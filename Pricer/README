Solution to http://www2.rgmadvisors.com/public/problems/orderbook/


1) I choose to do the project in Java, which is where most of my recent experience has been.  Java offers a nice compromise in terms of speed and safety and has a rich tool environment.  Java's garbage collection makes working with data structures simpler compared to reference counted STL containers in C++.  Both D and Go would have been interesting implementation languages for this project.  While both will likely perform better than my solution it would have taken me longer to come up with a correct program.  

2) The time complexity for an Add message is O(log n).  The order is added to a balanced tree, which takes O(log n) time.  The order id is also added to a hash map, which normally takes O(1) time.  My initial implementation then walked the tree to calculate the sum of the target size.  This original implementation took O(n) time for the aggregate function.  I refactored the code to maintain a sum within a red-black tree.  This allows sums to be calculated in O(log n) time as well.  The result is O(1) + 2*O(log n), which reduces to O(log n)  as constant factors are omitted in big O notation.

3) The time complexity for the Reduce operation is also O(log n).  Initially the order must be looked up in a hash map, which takes O(1).  The order is then found in the TreeMap.  The order is removed, reduced, and if re-inserted if needed.  The sum is then calculated which also takes O(log n) time.  The net results reduces to O(log n), same as the add operation. 

4) In general my approach to performance is to look for obviously incorrect data structures or algorithms, and then look at parallelization and distribution.  A typical midrange server may contain 16 cores.  Getting a program to use 16 cores may be easier and more economical than attempting to make the same program 16 times faster.  One usually only makes large performance gains when one finds an algorithm and reduces the time complexity.  For the Pricer program, each item can be processed in O(log n) time.  I suspect a solution may be possible using an vEB tree, performing the operations in O(log log n) time, however maintaining the target sum in O(log log n) time may require the help of Peter van Emde Boas.  Here's the checklist of things to do:
* Ensure the I/O system is fast enough.  Reading the data and writing the output should take a minimal amount of time.  The time waiting on the input pipeline or output pipeline should be so small as not to noticeable.  A slow NFS or ethernet could be a bottleneck, and should be remedied if it is.
* Replacing the BigDecimal with a double will improve the runtime performance on an Intel machine by about 25%.  I didn't make this optimization as BigDecimal's are often less error prone when dealing with money values than a floating point types.  If the code were to be run a machine like the Sparc T2000, ARM5, or similar FPU bound machine the double data type would run slower.  Changing the number type to double is a quick fix which could be done if needed.
* The JVM should be 1.6 or later.  Significant improvements were made between Sun's 1.5 JVM and 1.6 with regards to lock contention.  This is important as both the data reader thread and data writer thread have a lock which the main processing thread must contend with.  High lock contention could cause a performance bottleneck.
* Special code could be written to optimize for when the target size is 1.  This is because only the best bid/ask needs to be considered for this target.  An O(log log n) algorithm for a target size of 1 should be achievable.  
* The program uses three threads.  It's unlikely one could find further points for parallelizing the program for a single instrument.  It is likely the program is going to be running with hundreds or thousands of instruments.  In that case the work can easily be divided by instrument and run in parallel.  
